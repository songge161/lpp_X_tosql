# -*- coding: utf-8 -*-
"""
import_pgsql_to_mysql_entity_v8.6.py
-------------------------------------
å¢å¼ºç‰ˆï¼š
âœ… è‡ªåŠ¨é‡å‘½å temp_*.sqlï¼ˆåŸºäº DDL/INSERTï¼‰
âœ… ä¸­æ–‡è¡¨åæ”¯æŒã€UTF-8/GBK è‡ªé€‚åº”
âœ… æ¯æ¡è®°å½•ç”Ÿæˆ uuidï¼ˆå”¯ä¸€ï¼‰+ sidï¼ˆæ‰¹æ¬¡ï¼‰
âœ… uuid / sid ä¸ data å¹³çº§
âœ… å¤šçº¿ç¨‹å¯¼å…¥ + ç¼“å­˜åŒæ­¥ + å¹‚ç­‰æ§åˆ¶
âœ… å¤–éƒ¨æ¨¡å— custom_handler.py æ§åˆ¶ï¼š
   - æŒ‡å®šè¦å¤„ç†çš„è¡¨
   - æ¯è¡¨å­—æ®µå®šåˆ¶å¤„ç†é€»è¾‘
   - é»˜è®¤æ¸…æ´—é€»è¾‘ fallback
"""
import re, json, time, pymysql, traceback, threading, os, random, importlib
from pathlib import Path
from datetime import datetime
from typing import List, Any, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from backend.mapper_core import apply_record_mapping, get_target_entity, get_all_prioritized_tables, get_table_priority

# ========== é…ç½® ==========
SQL_DIR = "./source/sql"
TABLE_FILE = Path("./source/table_temp.txt")
CHANGE_FILE = Path("./source/change_temp.txt")
LOG_FILE = "import_log.txt"
THREADS = 6
DRY_RUN = False
DELETE_MODE = "physical"  # "logical" or "physical"
SID = "i6qzt3nn20"  # â† å…¨å±€å®å®šä¹‰ç©ºé—´
MYSQL_CFG = dict(
    host="127.0.0.1",
    port=3307,
    user="im",
    password="root",
    database="im",
    charset="utf8mb4",
    autocommit=False
)
# ==========================

# ---------- UUID ----------
_lock = threading.Lock()
_counter = 0
def base36_encode(n: int) -> str:
    chars = "0123456789abcdefghijklmnopqrstuvwxyz"
    s = ""
    while n:
        n, r = divmod(n, 36)
        s = chars[r] + s
    return s or "0"

def uuid() -> str:
    """å›ºå®š10ä½å”¯ä¸€çŸ­UUID"""
    global _counter
    with _lock:
        _counter = (_counter + 1) & 0xFF
    t = int(time.time()) % 1000
    us = int(time.time() * 1e6) % 100
    pid = os.getpid() % 100
    rnd = random.randint(0, 46655)
    n = (t << 24) | (us << 16) | (pid << 8) | _counter
    code = base36_encode(n) + base36_encode(rnd)
    return code[-10:].rjust(10, "0")

# ---------- å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ----------
try:
    custom_handler = importlib.import_module("custom_handler")
    ENABLED_TABLES = getattr(custom_handler, "ENABLED_TABLES", [])
    log_prefix = f"ğŸ”§ å·²åŠ è½½è‡ªå®šä¹‰æ¨¡å— custom_handler.pyï¼Œå¯ç”¨è¡¨ï¼š{ENABLED_TABLES}" if ENABLED_TABLES else "ğŸª¶ custom_handler.py åŠ è½½æˆåŠŸï¼ˆæœªé™åˆ¶è¡¨ï¼‰"
    print(log_prefix)
except ModuleNotFoundError:
    custom_handler = None
    ENABLED_TABLES = []
    print("âš ï¸ æœªæ‰¾åˆ° custom_handler.pyï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘ã€‚")

# ---------- æ­£åˆ™ ----------
INSERT_RE = re.compile(
    r"insert\s+into\s+public\.\"?(?P<table>[\w\u4e00-\u9fa5]+)\"?\s*\((?P<cols>[^)]*)\)\s*values\s*\((?P<vals>[^)]*)\)\s*;",
    re.IGNORECASE
)
DDL_RE = re.compile(
    r"create\s+table\s+(?:public\.)?\"?(?P<table>[\w\u4e00-\u9fa5]+)\"?",
    re.IGNORECASE
)

# ---------- å·¥å…· ----------
def safe_read_sql(file: Path) -> str:
    for enc in ("utf-8", "gbk", "utf-8-sig"):
        try:
            return file.read_text(encoding=enc)
        except Exception:
            continue
    return file.read_text(encoding="utf-8", errors="ignore")

def read_list(path: Path) -> List[str]:
    if not path.exists():
        return []
    raw = path.read_text(encoding="utf-8", errors="ignore")
    return [re.sub(r"[^\w\u4e00-\u9fa5_]", "", line.strip()) for line in raw.splitlines() if line.strip()]

def write_list(path: Path, data: List[str]):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text("\n".join(sorted(set(data))) + "\n", encoding="utf-8")

def to_timestamp(val: str) -> int:
    if not val:
        return int(time.time())
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
        try:
            return int(datetime.strptime(val, fmt).timestamp())
        except Exception:
            pass
    return int(time.time())

def log(msg: str):
    print(msg)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\n")

# ---------- è‡ªåŠ¨é‡å‘½å ----------
def try_rename_from_sql(file_path: Path) -> Path:
    text = safe_read_sql(file_path)
    m = DDL_RE.search(text) or INSERT_RE.search(text)
    if not m:
        return file_path
    inner_table = m.group("table").strip()
    new_path = file_path.parent / f"{inner_table}.sql"
    if new_path.name == file_path.name:
        return file_path
    if new_path.exists():
        log(f"[SKIP_RENAME] {file_path.name} â†’ {new_path.name} å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        return new_path
    file_path.rename(new_path)
    log(f"[RENAME] {file_path.name} â†’ {new_path.name}")
    return new_path

# ---------- SQLè§£æ ----------
def parse_values(raw: str) -> List[Any]:
    out, buf, in_str = [], [], False
    i = 0
    while i < len(raw):
        ch = raw[i]
        if in_str:
            if ch == "'":
                if i + 1 < len(raw) and raw[i + 1] == "'":
                    buf.append("'"); i += 2
                else:
                    in_str = False; i += 1
            else:
                buf.append(ch); i += 1
        else:
            if ch == "'": in_str = True; i += 1
            elif ch == ",": token = "".join(buf).strip(); out.append(_normalize(token)); buf=[]; i += 1
            else: buf.append(ch); i += 1
    token = "".join(buf).strip()
    out.append(_normalize(token))
    return out

def _normalize(token: str) -> Any:
    if token.upper() == "NULL": return ""
    if token.startswith("'") and token.endswith("'"): return token[1:-1].replace("''", "'")
    return token

def parse_sql_file(file_path: Path) -> List[Tuple[str, str, str, str, str, int, int, int]]:
    """
    è§£æ SQL æ–‡ä»¶ -> [(uuid, sid, type, name, data, del, input_ts, update_ts)]
    """
    text = safe_read_sql(file_path)
    matches = INSERT_RE.finditer(text)
    entities = []

    for m in matches:
        table = m.group("table")
        cols = [c.strip().strip('"') for c in m.group("cols").split(",")]
        vals = parse_values(m.group("vals"))
        if len(cols) != len(vals):
            continue

        record = dict(zip(cols, vals))
        deleted_val = record.get("deleted", "")
        create_time_val = record.get("create_time", "")
        update_time_val = record.get("update_time", "")
        for k in ("deleted", "create_time", "update_time"):
            record.pop(k, None)

        # --- custom_handler ---
        if custom_handler:
            func = getattr(custom_handler, table, None)
            if callable(func):
                try:
                    record = func(record) or record
                except Exception as e:
                    log(f"[WARN] {table} è‡ªå®šä¹‰å¤„ç†å¼‚å¸¸: {e}")
            elif hasattr(custom_handler, "default"):
                record = custom_handler.default(record, table)

        # --- åº”ç”¨ GUI é…ç½®è§„åˆ™ ---
        name_val = record.pop("__name__", "") or ""
        try:
            mapped_record, out_name, type_override = apply_record_mapping(table, record)
            if mapped_record:
                record = mapped_record
            if out_name:
                name_val = out_name
            if type_override:
                table = type_override
        except Exception as e:
            log(f"[WARN] {table} è§„åˆ™åº”ç”¨å¤±è´¥: {e}")

        # --- åºåˆ—åŒ– JSON ---
        data_json = json.dumps(record, ensure_ascii=False)
        uuid_val = uuid()
        entities.append((
            uuid_val,
            SID,
            table,
            name_val or "",   # ç¡®ä¿ name ä¸ä¸º None
            data_json,
            int(deleted_val or 0),
            to_timestamp(create_time_val),
            to_timestamp(update_time_val)
        ))

    return entities
# ---------- MySQL ----------
def ensure_table(conn):
    sql = """
    CREATE TABLE IF NOT EXISTS entity (
        id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
        uuid VARCHAR(64) NOT NULL,
        sid VARCHAR(64) NOT NULL,
        name VARCHAR(255) NOT NULL,
        type VARCHAR(128) NOT NULL,
        data JSON NOT NULL,
        del TINYINT DEFAULT 0,
        input_date BIGINT DEFAULT 0,
        update_date BIGINT DEFAULT 0,
        KEY idx_type (type),
        KEY idx_sid (sid)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    """
    with conn.cursor() as cur:
        cur.execute(sql)

def insert_entities(rows):
    conn = pymysql.connect(**MYSQL_CFG)
    try:
        ensure_table(conn)
        sql = """
        INSERT INTO entity
        (`uuid`, `sid`, `type`, `name`, `data`, `del`, `input_date`, `update_date`)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        with conn.cursor() as cur:
            cur.executemany(sql, rows)
        conn.commit()
    except Exception as e:
        conn.rollback()
        log(f"[ERROR_INSERT] {e}")
    finally:
        conn.close()

def handle_deleted_tables(del_arr: List[str], mode="logical"):
    if not del_arr: return
    conn = pymysql.connect(**MYSQL_CFG)
    with conn.cursor() as cur:
        for t in del_arr:
            if mode == "logical":
                cur.execute("UPDATE entity SET del=1 WHERE type=%s", (t,))
                log(f"ğŸŸ¡ è¡¨ {t} å·²é€»è¾‘åˆ é™¤")
            else:
                cur.execute("DELETE FROM entity WHERE type=%s", (t,))
                log(f"ğŸ—‘ï¸ è¡¨ {t} å·²ç‰©ç†åˆ é™¤")
        conn.commit()
    conn.close()

# ---------- çŠ¶æ€åŒæ­¥ ----------
def sync_state(data: List[str]):
    write_list(TABLE_FILE, data)
    write_list(CHANGE_FILE, data)
    log(f"ğŸ”„ çŠ¶æ€æ–‡ä»¶å·²åŒæ­¥ï¼Œå…± {len(data)} å¼ è¡¨ã€‚")

# ---------- ä¸»å¤„ç† ----------
def process_file(file_path: Path, allowed: Set[str]) -> str:
    try:
        real_path = try_rename_from_sql(file_path)
        table = real_path.stem
        if ENABLED_TABLES and table not in ENABLED_TABLES:
            log(f"[SKIP] {table} ä¸åœ¨ custom_handler å¯ç”¨åˆ—è¡¨ä¸­ã€‚")
            return None
        rows = parse_sql_file(real_path)
        if allowed and table not in allowed:
            log(f"[SKIP] {table} ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ã€‚")
            return None
        if not rows:
            log(f"[EMPTY] {table} æ— æœ‰æ•ˆè®°å½•ã€‚")
            return table
        insert_entities(rows)
        log(f"[OK] {table}: {len(rows)} æ¡å¯¼å…¥æˆåŠŸã€‚")
        return table
    except Exception as e:
        log(f"[ERROR] {file_path.name}: {e}\n{traceback.format_exc()}")
        return None

def compute_diff(table_state: List[str], user_state: List[str]) -> Tuple[List[str], List[str]]:
    ts, us = set(table_state), set(user_state)
    return list(us - ts), list(ts - us)

# ---------- ä¸»å…¥å£ ----------
def main():
    """
    ä¸»å…¥å£å‡½æ•°
    ----------------------------
    âœ… æ”¯æŒå¤šå±‚æ‰§è¡Œé¡ºåºï¼ˆåŸºç¡€è¡¨ â†’ æ™®é€šè¡¨ â†’ ä¾èµ–é‡è¡¨ï¼‰
    âœ… æ¯å±‚å†…éƒ¨å¤šçº¿ç¨‹å¹¶è¡Œ
    âœ… è‡ªåŠ¨è·³è¿‡å·²å¯¼å…¥è¡¨
    âœ… è‡ªåŠ¨æ›´æ–°ç¼“å­˜æ–‡ä»¶çŠ¶æ€
    """
    sql_dir = Path(SQL_DIR)
    all_files = sorted(sql_dir.glob("*.sql"))
    if not all_files:
        print("âŒ æœªæ‰¾åˆ° .sql æ–‡ä»¶")
        return

    # ---------- çŠ¶æ€åˆå§‹åŒ– ----------
    table_state = read_list(TABLE_FILE)
    user_state = read_list(CHANGE_FILE)
    unlimited = len(user_state) == 0

    if unlimited:
        log("âš™ï¸ æ— é™åˆ¶æ¨¡å¼ï¼šå¯¼å…¥æ‰€æœ‰æœªå¯¼å…¥è¡¨ã€‚")
        add_arr, del_arr = [], []
        allowed = set()
    else:
        add_arr, del_arr = compute_diff(table_state, user_state)
        allowed = set(user_state)
        if add_arr:
            log(f"â• æ–°å¢è¡¨: {add_arr}")
        if del_arr:
            log(f"ğŸ—‘ï¸ åˆ é™¤è¡¨: {del_arr}")
            handle_deleted_tables(del_arr, DELETE_MODE)

    imported = set()
    already = set(table_state)
    t0 = time.time()
    # åŠ¨æ€ä¼˜å…ˆçº§ï¼ˆæ¥è‡ª GUI é…ç½®ï¼‰ï¼Œæ•°å­—è¶Šå¤§è¶Šå…ˆæ‰§è¡Œ
    try:
        dyn_order = get_all_prioritized_tables()
        # ä½ åŸæ¥çš„é™æ€åˆ†å±‚è¿˜å¯ä¿ç•™åœ¨åé¢
    except Exception:
        dyn_order = []
    # ---------- å®šä¹‰åˆ†å±‚æ‰§è¡Œä¼˜å…ˆçº§ ----------
    priority_layers = [
        dyn_order,  # æ¥è‡ª GUI çš„ä¼˜å…ˆé˜Ÿåˆ—ï¼ˆå…ˆè·‘ï¼‰
        # ç¬¬ä¸€å±‚ï¼šåŸºç¡€è¡¨ï¼ˆä¼˜å…ˆæ‰§è¡Œï¼Œè¢«å¼•ç”¨æœ€å¤šï¼‰
        ["ct_fund_manage_firm", "ct_fund_base_info", "ct_fund_invest"],
        # ç¬¬äºŒå±‚ï¼šæ™®é€šè¡¨ï¼ˆæ— ç‰¹åˆ«ä¾èµ–ï¼‰
        [],
        # ç¬¬ä¸‰å±‚ï¼šå¼ºä¾èµ–ä¸­é—´è¡¨ï¼ˆæœ€åæ‰§è¡Œï¼‰
        ["ct_investor_fund_base", "ct_fund_firm_mid"],
    ]

    all_file_map = {f.stem: f for f in all_files}

    with ThreadPoolExecutor(max_workers=THREADS) as ex:
        for layer_id, layer_tables in enumerate(priority_layers, start=1):
            log(f"ğŸ§© å¼€å§‹ç¬¬ {layer_id} å±‚å¯¼å…¥ï¼š{layer_tables or 'è‡ªåŠ¨è¯†åˆ«'}")
            layer_tasks = []
            layer_imported = set()

            # å¦‚æœè¯¥å±‚ä¸ºç©º â†’ è‡ªåŠ¨é€‰å–æœªå¯¼å…¥ã€æœªåˆ†ç±»çš„è¡¨
            if not layer_tables:
                classified = sum(priority_layers, [])
                layer_tables = [
                    name for name in all_file_map.keys()
                    if name not in already and name not in classified
                ]

            # å±‚å†…ä»»åŠ¡åˆ†å‘
            for table_name in layer_tables:
                if table_name in already:
                    log(f"[SKIP] {table_name} å·²å¯¼å…¥ï¼Œè·³è¿‡ã€‚")
                    continue
                if allowed and table_name not in allowed:
                    continue
                f = all_file_map.get(table_name)
                if not f:
                    log(f"[WARN] æœªæ‰¾åˆ° {table_name}.sqlï¼Œè·³è¿‡ã€‚")
                    continue
                layer_tasks.append(ex.submit(process_file, f, allowed))

            # ç­‰å¾…å±‚å†…ä»»åŠ¡å®Œæˆ
            for fut in as_completed(layer_tasks):
                r = fut.result()
                if r:
                    imported.add(r)
                    layer_imported.add(r)

            log(f"âœ… ç¬¬ {layer_id} å±‚å®Œæˆï¼šå…± {len(layer_imported)} å¼ è¡¨ã€‚")

    # ---------- åŒæ­¥æœ€ç»ˆçŠ¶æ€ ----------
    final_state = sorted((set(table_state) - set(del_arr)) | imported | set(add_arr))
    sync_state(final_state)

    # ---------- å®Œæˆæ—¥å¿— ----------
    log(
        f"âœ… åŒæ­¥å®Œæˆï¼Œå…± {len(final_state)} å¼ è¡¨ï¼Œ"
        f"æ–°å¢ {len(imported)} å¼ ï¼Œç”¨æ—¶ {time.time()-t0:.2f}s"
    )
    print(
        f"\nâœ… å¯¼å…¥æµç¨‹å®Œæˆï¼š\n"
        f"  æ€»è¡¨æ•°ï¼š{len(final_state)}\n"
        f"  æ–°å¢å¯¼å…¥ï¼š{len(imported)}\n"
        f"  åˆ é™¤è¡¨ï¼š{len(del_arr)}\n"
        f"  æ€»è€—æ—¶ï¼š{time.time()-t0:.2f}s\n"
    )


# ---------- è¡¨çŠ¶æ€æ›´æ–°ï¼ˆå¢å¼º+æ•°æ®å¤„ç†ï¼‰ ----------
def update_table_list(add_arr=None, del_arr=None, sync_db=True, process_data=True):
    """
    è¡¨çŠ¶æ€æ›´æ–°å‡½æ•°ï¼ˆæ–‡ä»¶ + æ•°æ®åº“ + å¯é€‰æ•°æ®å¤„ç†ï¼‰
    ------------------------------------------------------------
    âœ… é»˜è®¤è‡ªåŠ¨åŒæ­¥æ•°æ®åº“
    âœ… å¯é€‰å‚æ•° process_data=Trueï¼šæ–°å¢è¡¨æ—¶è‡ªåŠ¨å¯¼å…¥ SQL æ•°æ®å¹¶æ‰§è¡Œæ¸…æ´—
    âœ… æ”¯æŒå¢åˆ å¹¶è¡Œ
    âœ… è‡ªåŠ¨å†™æ—¥å¿—ã€æ‰“å°çŠ¶æ€
    """
    add_arr = add_arr or []
    del_arr = del_arr or []

    table_state = read_list(TABLE_FILE)
    change_state = read_list(CHANGE_FILE)
    table_set = set(table_state)
    change_set = set(change_state)

    before_table, before_change = len(table_set), len(change_set)

    # --- å¢åŠ  ---
    added = []
    for t in add_arr:
        t = t.strip()
        if not t:
            continue
        if t not in table_set:
            table_set.add(t)
            change_set.add(t)
            added.append(t)

    # --- åˆ é™¤ ---
    removed = []
    for t in del_arr:
        t = t.strip()
        if not t:
            continue
        if t in table_set:
            table_set.discard(t)
            change_set.discard(t)
            removed.append(t)

    # --- å†™æ–‡ä»¶åŒæ­¥ ---
    write_list(TABLE_FILE, sorted(table_set))
    write_list(CHANGE_FILE, sorted(change_set))
    log(f"[UPDATE_LIST] add={added}, del={removed}, total={len(table_set)}")

    print(f"âœ… æ–‡ä»¶çŠ¶æ€å·²æ›´æ–°ï¼š+{len(added)} / -{len(removed)}")
    print(f"ğŸ“ å½“å‰è¡¨æ•°é‡ï¼štable_temp={len(table_set)} (åŸ {before_table}) | change_temp={len(change_set)} (åŸ {before_change})")

    # --- åˆ é™¤æ•°æ®åº“ ---
    if sync_db and removed:
        try:
            handle_deleted_tables(removed, DELETE_MODE)
            log(f"[SYNC_DB] å·²åŒæ­¥æ•°æ®åº“åˆ é™¤ {removed}")
            print(f"ğŸ—‘ï¸ å·²åŒæ­¥åˆ é™¤æ•°æ®åº“ä¸­ {len(removed)} å¼ è¡¨ã€‚")
        except Exception as e:
            log(f"[ERROR_SYNC_DB] {e}")
            print(f"âš ï¸ åŒæ­¥æ•°æ®åº“åˆ é™¤å¤±è´¥: {e}")

    # --- æ–°å¢è¡¨æ•°æ®å¯¼å…¥ ---
    if process_data and added:
        print(f"âš™ï¸ æ­£åœ¨å¤„ç†æ–°å¢è¡¨æ•°æ®ï¼š{added}")
        sql_dir = Path(SQL_DIR)
        all_files = {f.stem: f for f in sql_dir.glob("*.sql")}
        for t in added:
            f = all_files.get(t)
            if not f:
                log(f"[WARN] æœªæ‰¾åˆ° {t}.sql æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
                continue
            try:
                table_name = process_file(f, allowed=set([t]))
                if table_name:
                    log(f"[PROCESS_OK] {t} å·²é‡æ–°å¯¼å…¥ã€‚")
                    print(f"âœ… {t} å·²é‡æ–°å¯¼å…¥ã€‚")
            except Exception as e:
                log(f"[ERROR_PROCESS] {t}: {e}")
                print(f"âš ï¸ {t} å¯¼å…¥å¤±è´¥: {e}")

    return list(sorted(table_set))

if __name__ == "__main__":
    main()
