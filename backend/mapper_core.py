# backend/mapper_core.py
# -*- coding: utf-8 -*-
from typing import Dict, Any, Tuple, Optional, List, Callable
import re, pymysql, json, time,math, datetime
from pathlib import Path
from types import SimpleNamespace
from backend.db import list_tables, get_priority, get_field_mappings, get_target_entity
from backend.source_fields import detect_sql_path
from backend.sql_utils import get_conn, is_pg, json_equals_clause

try:
    from version3 import MYSQL_CFG, SID
except Exception:
    MYSQL_CFG = dict(
        host="127.0.0.1", port=3307, user="im", password="root",
        database="im", charset="utf8mb4", autocommit=False
    )
    SID = "default_sid"

class SafeRecord(SimpleNamespace):
    def get(self, key, default=None):
        return getattr(self, key, default)

_CACHE: Dict[str, Any] = {}
# ä¸“ç”¨äº SQL æºæ–‡ä»¶è§£æçš„ç¼“å­˜ï¼ˆä¸éšæ¯æ¡æ˜ å°„æ¸…ç©ºï¼‰
_SQL_ROWS_CACHE: Dict[str, List[Dict[str, Any]]] = {}
_SQL_IDX_CACHE: Dict[str, Dict[str, Dict[str, Any]]] = {}

COMPLEX_EXPR_RE = re.compile(
    r"(?P<etype>(entity|sql))\.(?P<table>\w+)\(\s*(?P<cond>.+?)\s*\)\.(?P<target>[\w\.]+)",
    re.S
)

def _eval_complex_expr(expr: str, record: Dict[str, Any]) -> Optional[Any]:
    expr = re.sub(r"\s+", " ", expr.strip())
    m = COMPLEX_EXPR_RE.fullmatch(expr)
    if not m:
        return None

    etype, table, cond, target = (
        m.group("etype"), m.group("table"), m.group("cond"), m.group("target")
    )

    if "=" not in cond:
        return None
    left, right = [x.strip() for x in cond.split("=", 1)]

    # é€’å½’å³å€¼
    if right.startswith(("entity.", "sql.")):
        right_val = _eval_complex_expr(right, record)
    else:
        right_val = _eval_rule(right, record)

    if right_val in (None, "", "null", "NULL"):
        return None

    where_field = left.split(".")[-1].replace("data.", "")
    # æ ¹æ®å‰ç¼€åˆ†æµï¼šentity -> æŸ¥è¯¢å®ä½“è¡¨ï¼›sql -> æŸ¥æº SQL æ–‡ä»¶
    if etype == "sql":
        return _sql_lookup(table, where_field, right_val, target)
    return _entity_fetch(table, where_field, right_val, target)
# ==================== ğŸ”§ SQL ç›´æ¥æŸ¥è¯¢è¡¥ä¸ ====================

SQL_COMPLEX_RE = re.compile(
    r"sql\.(?P<table>\w+)\(\s*sql\.(?P<table2>\w+)\.(?P<where_field>[\w\.]+)\s*=\s*(?P<sexpr>[^)]+)\s*\)\.(?P<target>[\w\.]+)",
    re.S
)

def _eval_sql_complex_expr(expr: str, record: Dict[str, Any]) -> Optional[Any]:
    """
    æ”¯æŒç±»ä¼¼ï¼š
        sql.import_fund_info(sql.import_fund_info.fund_id=record.id).department
    çš„ç»“æ„ï¼Œç›´æ¥ä»æº SQL æ–‡ä»¶ä¸­æŸ¥å­—æ®µã€‚
    """
    m = SQL_COMPLEX_RE.fullmatch(expr.strip())
    if not m:
        return None

    table = m.group("table")
    where_field = m.group("where_field").replace("data.", "")
    src_expr = m.group("sexpr").strip()
    target_field = m.group("target")

    # è®¡ç®—å³å€¼
    v = None
    if src_expr.startswith("record."):
        v = record.get(src_expr[7:], "")
    elif src_expr in record:
        v = record.get(src_expr, "")
    else:
        try:
            from types import SimpleNamespace
            rec_obj = SafeRecord(**{k: v for k, v in record.items()})
            v = eval(src_expr, {"__builtins__": {}}, {"record": rec_obj, **record})
        except Exception:
            v = src_expr
    if not v:
        return ""

    # ç›´æ¥æŸ¥ SQL
    try:
        return _sql_lookup(table, where_field, v, target_field) or ""
    except Exception as e:
        print("[_eval_sql_complex_expr error]", e)
        return ""

def __date_ts__(v) -> int:
    """
    é€šç”¨æ—¶é—´è½¬ç§’çº§æ—¶é—´æˆ³ï¼š
      - æ”¯æŒå­—ç¬¦ä¸²ï¼ˆå«æ¯«ç§’ï¼‰: "2025-08-13 12:49:06.688000"
      - æ”¯æŒçº¯æ—¥æœŸ: "2025-08-13"
      - æ”¯æŒæ•°å­—(ç§’/æ¯«ç§’): 1699911111000 æˆ– 1699911111
      - ä¸ºç©ºè¿”å›å½“å‰æ—¶é—´æˆ³
    """
    import datetime, time
    if not v:
        return int(time.time())
    if isinstance(v, (int, float)):
        # æ¯«ç§’ -> ç§’
        return int(v // 1000 if v > 1e12 else v)
    if isinstance(v, str):
        s = v.strip()
        # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
        for fmt in ("%Y-%m-%d %H:%M:%S.%f", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
            try:
                return int(datetime.datetime.strptime(s.split(".")[0], fmt).timestamp())
            except Exception:
                continue
        # çº¯æ•°å­—å­—ç¬¦ä¸²
        if s.isdigit():
            return int(int(s) // 1000 if int(s) > 1e12 else int(s))
    return int(time.time())
# ================= å®‰å…¨ Entity Fetch =================
def _entity_fetch(type_name: str, where_field: str, where_val: Any, target_path: str) -> Optional[Any]:
    """å®‰å…¨çš„ entity æŸ¥è¯¢ï¼šæœªå‘½ä¸­è¿”å› Noneï¼Œä¸å¤ç”¨æ—§ç¼“å­˜ã€‚
    å…¼å®¹ MySQL ä¸ PostgreSQLã€‚
    """
    key = f"E:{type_name}:{where_field}={where_val}:{target_path}"
    conn = None
    try:
        conn = get_conn()
        with conn.cursor() as cur:
            if target_path == "uuid":
                sql = f"SELECT uuid FROM entity WHERE type=%s AND {json_equals_clause('data', where_field)} LIMIT 1"
                cur.execute(sql, (type_name, str(where_val)))
            else:
                # ç›®æ ‡è·¯å¾„ç»Ÿä¸€ä» data JSON é‡Œå–æ–‡æœ¬
                tpath = target_path.replace("data.", "")
                if is_pg():
                    sql = f"SELECT data->>'{tpath}' FROM entity WHERE type=%s AND {json_equals_clause('data', where_field)} LIMIT 1"
                    cur.execute(sql, (type_name, str(where_val)))
                else:
                    sql = f"SELECT JSON_UNQUOTE(JSON_EXTRACT(data, '$.{tpath}')) FROM entity WHERE type=%s AND {json_equals_clause('data', where_field)} LIMIT 1"
                    cur.execute(sql, (type_name, str(where_val)))
            row = cur.fetchone()
            if row and row[0]:
                _CACHE[key] = row[0]
                return row[0]
            else:
                if key in _CACHE:
                    del _CACHE[key]
                return None
    except Exception as e:
        print("[_entity_fetch error]", e)
        return None
    finally:
        if conn:
            conn.close()


# ================= entity(...) JOIN æ¨¡å¼ =================
ENTITY_JOIN_RE = re.compile(
    r"entity\("
    r"(?P<target>\w+):"
    r"(?P<tfield>[\w\.]+)="
    r"(?P<sexpr>"                  # å³å€¼æ”¯æŒä¸¤ç±»ï¼š
    r"entity\([^)]*\)\.[\w\.]+"    #   a) entity(...).path
    r"|[\w\.]+"                    #   b) record å­—æ®µæˆ– a.b.c é“¾
    r")\)"
    r"\.(?P<path>[\w\.]+)"
)

def _eval_entity_join(expr: str, record: Dict[str, Any]) -> Optional[Any]:
    m = ENTITY_JOIN_RE.fullmatch(expr.strip())
    if not m:
        return None

    target_tbl  = m.group("target")
    target_field = m.group("tfield")            # ä¾‹å¦‚ data.id
    source_expr = m.group("sexpr")              # å¯èƒ½æ˜¯ "record.xxx" / "id" / "entity(...).yyy"
    target_path = m.group("path")               # ä¾‹å¦‚ uuid / data.xxx

    # 1) å³å€¼å¦‚æœæ˜¯å†…å±‚ entity(...) è¡¨è¾¾å¼ï¼Œå…ˆé€’å½’æ±‚å€¼
    if source_expr.strip().startswith("entity("):
        # ç›´æ¥å¤ç”¨ rule æ±‚å€¼å¼•æ“ï¼Œæ‹¿åˆ°å®é™…å³å€¼
        inner_val = _eval_rule(source_expr, record)
        if inner_val in (None, "", "null", "NULL"):
            return None
        return _entity_fetch(
            target_tbl,
            target_field.replace("data.", ""),
            inner_val,
            target_path
        )

    # 2) å¦åˆ™æŒ‰åŸå…ˆé€»è¾‘ï¼šä» record é‡Œå–å³å€¼ï¼ˆæ”¯æŒ record.xxx æˆ– a.b.c é“¾ï¼‰
    parts = source_expr.split(".")
    v = record
    for seg in parts:
        if isinstance(v, dict):
            v = v.get(seg)
        else:
            v = None
            break
    if v is None:
        return None

    return _entity_fetch(
        target_tbl,
        target_field.replace("data.", ""),
        v,
        target_path
    )


# ================= ç›´æ¥æŒ‰ type+id å–å€¼ï¼ˆå¯é€‰ï¼‰ =================
def _entity_rel_fetch(type_name: str, entity_id: Any, field: str = "uuid") -> Optional[Any]:
    """entity_rel(ct_company_info,12345)"""
    key = f"REL:{type_name}:{entity_id}:{field}"
    if key in _CACHE:
        return _CACHE[key]
    conn = pymysql.connect(**MYSQL_CFG)
    try:
        with conn.cursor() as cur:
            if field == "uuid":
                cur.execute("SELECT uuid FROM entity WHERE type=%s AND JSON_UNQUOTE(JSON_EXTRACT(data,'$.id'))=%s LIMIT 1", (type_name, entity_id))
            else:
                cur.execute("SELECT JSON_UNQUOTE(JSON_EXTRACT(data,%s)) FROM entity WHERE type=%s AND JSON_UNQUOTE(JSON_EXTRACT(data,'$.id'))=%s LIMIT 1",
                            (f"$.{field}", type_name, entity_id))
            row = cur.fetchone()
            if row and row[0]:
                _CACHE[key] = row[0]
                return row[0]
    except Exception as e:
        print("[_entity_rel_fetch error]", e)
    finally:
        conn.close()
    return None

# ================= æº SQL æŸ¥æ‰¾ï¼ˆç”¨äº sql.xxx(...)ï¼‰ =================
def _sql_lookup(table: str, where_field: str, where_val: Any, target_field: str) -> Optional[Any]:
    """åœ¨ source/sql/<table>.sql ä¸­æŒ‰åˆ—åŒ¹é…å¹¶è¿”å›ç›®æ ‡åˆ—å€¼ã€‚
    - è§£æ INSERT è¯­å¥å¾—åˆ°è¡Œåˆ—è¡¨ï¼›ç®€å•æ‰«æåŒ¹é… where_field==where_valï¼›è¿”å› target_field
    - è‹¥æ–‡ä»¶ä¸å­˜åœ¨æˆ–æœªå‘½ä¸­ï¼Œè¿”å› None
    - ä½¿ç”¨è¿›ç¨‹çº§ç¼“å­˜é¿å…é‡å¤è§£æ
    """
    try:
        wf = str(where_field or "").replace("data.", "").strip()
        tf = str(target_field or "").strip()
        tv = str(where_val or "").strip()

        # å– rows ç¼“å­˜ï¼ˆä¸å— _CACHE.clear() å½±å“ï¼‰
        rows = _SQL_ROWS_CACHE.get(table)
        if rows is None:
            p = detect_sql_path(table)
            if not p.exists():
                return None
            rows = _parse_sql_file(p)
            _SQL_ROWS_CACHE[table] = rows

        # è‹¥è¯¥å­—æ®µæœªå»ºç«‹ç´¢å¼•ï¼Œæ„å»ºä¸€æ¬¡åŸºäºå°å†™ã€å»ç©ºæ ¼çš„ç´¢å¼•
        idx_key = f"{table}|{wf}"
        idx = _SQL_IDX_CACHE.get(idx_key)
        if idx is None:
            idx = {}
            for r in rows:
                rv = str(r.get(wf, "")).strip().lower()
                if rv:
                    # è‹¥å­˜åœ¨é‡å¤é”®ï¼Œä¿ç•™ç¬¬ä¸€æ¡ï¼›å¦‚éœ€æœ€æ–°ç­–ç•¥å¯å¦åŠ é€»è¾‘
                    idx.setdefault(rv, r)
            _SQL_IDX_CACHE[idx_key] = idx

        hit = idx.get(tv.strip().lower())
        if not hit:
            return None
        val = hit.get(tf, None)
        if val in (None, "", "NULL", "null"):
            return None
        return val
    except Exception as e:
        print("[_sql_lookup error]", e)
        return None

# ========== å¯¹å¤–ï¼šSQL ç¼“å­˜ç®¡ç† ==========
def clear_sql_cache(table: Optional[str] = None) -> Dict[str, int]:
    """æ¸…ç† SQL è§£æä¸ç´¢å¼•ç¼“å­˜ã€‚table ä¸ºç©ºåˆ™æ¸…ç†å…¨éƒ¨ã€‚è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    cleared_rows = 0
    cleared_idx = 0
    if table:
        if table in _SQL_ROWS_CACHE:
            del _SQL_ROWS_CACHE[table]
            cleared_rows = 1
        # åˆ é™¤è¯¥è¡¨çš„æ‰€æœ‰å­—æ®µç´¢å¼•
        keys = [k for k in _SQL_IDX_CACHE.keys() if k.startswith(f"{table}|")]
        for k in keys:
            del _SQL_IDX_CACHE[k]
            cleared_idx += 1
    else:
        cleared_rows = len(_SQL_ROWS_CACHE)
        cleared_idx = len(_SQL_IDX_CACHE)
        _SQL_ROWS_CACHE.clear()
        _SQL_IDX_CACHE.clear()
    return {"rows": cleared_rows, "idx": cleared_idx}

def warm_sql_cache(tables: List[str]) -> Dict[str, int]:
    """é¢„çƒ­æŒ‡å®šè¡¨çš„ SQL è§£æä¸ç´¢å¼•ç¼“å­˜ã€‚è¿”å›æˆåŠŸé¢„çƒ­çš„è¡¨æ•°é‡å’Œç´¢å¼•æ•°é‡ã€‚"""
    warmed_rows = 0
    warmed_idx = 0
    for tbl in tables or []:
        try:
            p = detect_sql_path(tbl)
            if not p.exists():
                continue
            rows = _SQL_ROWS_CACHE.get(tbl)
            if rows is None:
                rows = _parse_sql_file(p)
                _SQL_ROWS_CACHE[tbl] = rows
                warmed_rows += 1
            # ä¸ºæ¯ä¸ªå­—æ®µå»ºç«‹ä¸€æ¬¡ç´¢å¼•ï¼ˆæŒ‰éœ€å¯é™åˆ¶å­—æ®µé›†åˆï¼‰
            if rows:
                # æŒ‰é¦–æ¡è®°å½•çš„é”®é›†åˆå»ºç«‹ç´¢å¼•ï¼Œé¿å…å…¨è¡¨æ‰€æœ‰åˆ—å¸¦æ¥è¿‡å¤šç´¢å¼•
                sample_keys = list(rows[0].keys())
                for wf in sample_keys:
                    idx_key = f"{tbl}|{wf}"
                    if idx_key in _SQL_IDX_CACHE:
                        continue
                    idx = {}
                    for r in rows:
                        rv = str(r.get(wf, "")).strip().lower()
                        if rv:
                            idx.setdefault(rv, r)
                    _SQL_IDX_CACHE[idx_key] = idx
                    warmed_idx += 1
        except Exception:
            continue
    return {"rows": warmed_rows, "idx": warmed_idx}


# ================= Python å®‰å…¨æ±‚å€¼ï¼ˆpy:{...}ï¼‰ =================
FUNC_COALESCE = re.compile(r"coalesce\((.+)\)$", re.I)
FUNC_CONCAT   = re.compile(r"concat\((.+)\)$", re.I)
PY_RE         = re.compile(r"py:(?P<expr>.+)", re.S)
ENTITY_SIMPLE_RE = re.compile(r"entity\((?P<typ>\w+)(?:,by=(?P<by>\w+))?(?:,src=(?P<src>\w+))?\)\.(?P<path>[\w\.]+)")
REL_RE           = re.compile(r"rel\((?P<typ>\w+)(?:,by=(?P<by>\w+))?(?:,src=(?P<src>\w+))?\)$")
SOURCE_RE        = re.compile(r"source\((?P<table>\w+)\.(?P<field>\w+)=(?P<sexpr>[\w\.]+)\)\.(?P<target>\w+)")


def _split_args(s: str) -> List[str]:
    return [a.strip() for a in s.split(",")]
# ========== æ—¶é—´æˆ³è¾…åŠ© ==========



def _eval_atom(atom: str, record: Dict[str, Any]) -> Any:
    a = atom.strip()
    if not a:
        return ""
    if a.startswith("'") and a.endswith("'"):
        return a[1:-1]
    if a.startswith("record."):
        return record.get(a[7:], "")
    if a.startswith("entity("):
        v = _eval_entity_join(a, record)
        # âœ… None æˆ–ç©ºå­—ç¬¦ä¸²éƒ½ç®—â€œæ²¡å‘½ä¸­â€
        if v not in (None, "null", "NULL"):
            return v
    if a in record:
        return record.get(a)
    return a


def _eval_rule(rule: str, record: Dict[str, Any]) -> Any:
    import ast, datetime
    from types import SimpleNamespace

    r = (rule or "").strip()
    if not r:
        return None
    # --- ä¼˜å…ˆæ£€æµ‹ entity/sql åµŒå¥—è¡¨è¾¾å¼ ---
    m = COMPLEX_EXPR_RE.fullmatch(r)
    if m:
        return _eval_complex_expr(r, record)
    # âœ… æ”¯æŒ sql.xxx(...) é¡¶å±‚ç»“æ„
    m = SQL_COMPLEX_RE.fullmatch(r)
    if m:
        return _eval_sql_complex_expr(r, record)
    # ========== âœ… æ–°å¢: date(fmt, field) æˆ– date(fmt, record.xxx) ==========
    # æ”¯æŒä¸¤ç§å†™æ³•ï¼š
    #   date(%Y-%m-%d, ic_register_date)
    #   date(%Y-%m-%d, record.fund_record_time)
    if r.lower().startswith("date(") and "," in r:
        try:
            # æå–å‚æ•°éƒ¨åˆ†
            inner = r[5:-1]
            fmt_part, field_part = [x.strip() for x in inner.split(",", 1)]
            fmt = fmt_part.strip()
            if fmt.startswith("'") or fmt.startswith('"'):
                fmt = fmt[1:-1]
            # å–å€¼
            if field_part.startswith("record."):
                val = record.get(field_part[7:], "")
            else:
                val = record.get(field_part, "")
            if not val:
                return ""
            # æ ¼å¼åŒ–
            for f in ("%Y-%m-%d %H:%M:%S.%f", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
                try:
                    dt = datetime.datetime.strptime(str(val).split(".")[0], f)
                    return dt.strftime(fmt)
                except Exception:
                    continue
            return str(val).split(" ")[0]
        except Exception as e:
            print("[date(fmt, field) error]", e)
            return ""

    # ========== âœ… ä¿ç•™: date:%Y-%m-%d æ—§å†™æ³• ==========
    if r.startswith("date:"):
        fmt = r[5:].strip()
        val = record.get("fund_record_time") or record.get("date") or ""
        if not val:
            return ""
        try:
            for f in ("%Y-%m-%d %H:%M:%S.%f", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
                try:
                    dt = datetime.datetime.strptime(str(val).split(".")[0], f)
                    return dt.strftime(fmt)
                except Exception:
                    continue
        except Exception:
            pass
        return str(val).split(" ")[0]
    # ========== coalesce(...) ==========
    m = FUNC_COALESCE.match(r)
    if m:
        for x in _split_args(m.group(1)):
            v = _eval_atom(x, record)
            if v not in (None, "", "null", "NULL"):
                return v
        return ""

    # ========== concat(...) ==========
    m = FUNC_CONCAT.match(r)
    if m:
        return "".join(str(_eval_atom(x, record) or "") for x in _split_args(m.group(1)))

    # ========== entity ç®€å• ==========
    m = ENTITY_SIMPLE_RE.fullmatch(r)
    if m:
        typ, by, src, path = m.group("typ"), m.group("by") or "id", m.group("src"), m.group("path")
        src_val = record.get(src or f"{typ}_id") or record.get("id")
        if src_val is None:
            return None
        return _entity_fetch(typ, by, src_val, path)

    # ========== entity join ==========
    m = ENTITY_JOIN_RE.fullmatch(r)
    if m:
        target_tbl, tfield, sexpr, target_path = m.group("target"), m.group("tfield"), m.group("sexpr"), m.group("path")
        src_val = _eval_atom(sexpr, record)
        if src_val is None:
            return None
        return _entity_fetch(target_tbl, tfield.replace("data.", ""), src_val, target_path)

    # ========== rel(...) ==========
    m = REL_RE.fullmatch(r)
    if m:
        typ, by, src = m.group("typ"), m.group("by") or "id", m.group("src")
        src_val = record.get(src or f"{typ}_id") or record.get("id")
        if src_val is None:
            return None
        return _entity_fetch(typ, by, src_val, "uuid")

    # ========== source(...) ==========
    m = SOURCE_RE.fullmatch(r)
    if m:
        table, field, sexpr, target = m.group("table"), m.group("field"), m.group("sexpr"), m.group("target")
        src_val = _eval_atom(sexpr, record)
        return f"[source:{table}.{field}={src_val}->{target}]"

    # ========== py:{...} ==========
    m = PY_RE.match(r)
    if m:
        expr = m.group("expr").strip()
        rec_obj = SafeRecord(**{k: v for k, v in record.items()})
        # --- py:{...}.get(...) å­—å…¸æ˜ å°„ ---
        if ".get(" in expr and expr.strip().startswith("{"):
            try:
                dict_part, _, tail = expr.partition("}.get(")
                dict_part = dict_part + "}"
                args_part = tail.rsplit(")", 1)[0]
                mapping = ast.literal_eval(dict_part)
                args = [a.strip() for a in args_part.split(",")]
                key_expr = args[0]
                default_val = ast.literal_eval(args[1]) if len(args) > 1 else ""

                def _eval_key(_e: str):
                    _e = _e.strip()
                    if _e.startswith("record."):
                        return record.get(_e[7:], "")
                    if _e in record:
                        return record[_e]
                    return _e

                raw_val = _eval_key(key_expr)
                if isinstance(raw_val, str) and "," in raw_val:
                    parts = [p.strip() for p in raw_val.split(",") if p.strip()]
                    mapped = [mapping.get(p, default_val) for p in parts]
                    return ",".join(mapped)
                return mapping.get(str(raw_val), default_val)
            except Exception as e:
                print("[py-get parse error]", e)
                return None

        # --- å¸¸è§„ py è¡¨è¾¾å¼ ---
        try:
            safe_globals = {
                "__builtins__": {
                    "str": str, "int": int, "float": float, "len": len, "round": round,
                    "dict": dict, "list": list, "__date_ts__": __date_ts__,
                },
                "re": re,
                "json": json,
                "__date_ts__": __date_ts__
            }
            val = eval(expr, safe_globals, {"record": rec_obj, **record})
            return val
        except Exception as e:
            print("[py expr error]", e)
            return None

    # ========== é»˜è®¤ ==========
    return _eval_atom(r, record)



# ================= åº”ç”¨æ˜ å°„ =================
def apply_record_mapping(source_table: str, record: Dict[str, Any], py_script: str = "", target_entity: Optional[str] = None) -> Tuple[Dict[str, Any], str, str]:
    # æ¯æ¬¡æ˜ å°„å‰æ¸…ç©º Entity ç¼“å­˜ï¼Œé¿å…è„ç¼“å­˜
    _CACHE.clear()

    # æŒ‰å½“å‰ entity è¿‡æ»¤å­—æ®µæ˜ å°„ï¼›æœªæŒ‡å®šåˆ™ä½¿ç”¨è¡¨é»˜è®¤
    mappings = get_field_mappings(source_table, target_entity or None)
    type_override = (target_entity or get_target_entity(source_table) or "")
    out_name = ""
    new_rec = dict(record)

    for m in mappings:
        if not int(m["enabled"]):
            continue

        targets = [t.strip() for t in (m["target_paths"] or "").split(",") if t.strip()]
        rule = (m["rule"] or "").strip()
        src = m["source_field"] or ""

        # ---------- è§„åˆ™æ±‚å€¼ ----------
        if "||" in rule and len(targets) > 1:
            # å¤šè§„åˆ™åˆ†å‘ï¼šrule_a || rule_b -> å¯¹åº”å¤šä¸ª target
            sub_rules = [x.strip() for x in rule.split("||")]
            for i, t in enumerate(targets):
                sel_rule = sub_rules[i] if i < len(sub_rules) else sub_rules[-1]
                val_i = _eval_rule(sel_rule, new_rec)

                # âœ… å…³é”®ç‚¹ï¼šåªè¦å†™äº† ruleï¼Œå°±ä¸å…è®¸å›é€€åˆ° source_field
                if val_i is None:
                    val_i = ""   # ä¸å›é€€åˆ° new_rec.get(src, "")

                _assign_target(new_rec, t, val_i, name_holder=lambda v: _set_name(new_rec, v))
        else:
            # å•è§„åˆ™æˆ–æ— è§„åˆ™
            if rule:
                val = _eval_rule(rule, new_rec)

                # âœ… å…³é”®ç‚¹ï¼šåªè¦å†™äº† ruleï¼Œå°±ä¸å…è®¸å›é€€åˆ° source_field
                if val is None:
                    val = ""    # ä¸å›é€€

            else:
                # æ²¡å†™ ruleï¼Œä¿æŒâ€œé€ä¼ å›é€€â€åˆ°æºå­—æ®µ
                val = new_rec.get(src, "")

            for t in targets:
                _assign_target(new_rec, t, val, name_holder=lambda v: _set_name(new_rec, v))

    if "__name__" not in new_rec:
        new_rec["__name__"] = out_name or new_rec.get("__name__", "")

    # ---------- è¡¨çº§è„šæœ¬ ----------
    if py_script and py_script.strip():
        try:
            safe_globals = {
                "__builtins__": {
                    "len": len, "str": str, "int": int, "float": float,
                    "dict": dict, "list": list, "print": print,
                    "range": range, "__import__": __import__,
                }
            }
            # æ³¨å…¥å½“å‰ entity ä¸Šä¸‹æ–‡ï¼Œè„šæœ¬å¯è¯» current_entity / type_name
            loc = {
                "record": new_rec,
                "current_entity": (target_entity or ""),
                "target_entity": (target_entity or ""),
                "type_name": (target_entity or get_target_entity(source_table) or source_table)
            }
            exec(py_script, safe_globals, loc)
            new_rec = loc["record"]
        except Exception as e:
            print("[py_script error]", e)

    return new_rec, new_rec.get("__name__", ""), type_override



def _set_name(rec: Dict[str, Any], v: Any):
    rec["__name__"] = str(v or "")


def _assign_target(rec: Dict[str, Any], t: str, val: Any, name_holder=None):
    if t == "name":
        if name_holder:
            name_holder(val)
        else:
            rec["__name__"] = str(val or "")
        return
    if t.startswith("data."):
        path = t[5:].split(".")
        cur = rec
        for seg in path[:-1]:
            cur = cur.setdefault(seg, {})
        cur[path[-1]] = val
        return
    rec[t] = val


# ==================== SQL è§£æ & å…¥åº“å·¥å…· ====================
INSERT_RE = re.compile(
    r"insert\s+into\s+public\.\"?(?P<table>[\w\u4e00-\u9fa5]+)\"?\s*\((?P<cols>[^)]*)\)\s*values\s*\((?P<vals>[^)]*)\)\s*;",
    re.IGNORECASE
)

def _safe_read_sql(file: Path) -> str:
    for enc in ("utf-8", "gbk", "utf-8-sig"):
        try:
            return file.read_text(encoding=enc)
        except Exception:
            continue
    return file.read_text(encoding="utf-8", errors="ignore")

def _parse_values(raw: str) -> List[Any]:
    out, buf, in_str = [], [], False
    i = 0
    while i < len(raw):
        ch = raw[i]
        if in_str:
            if ch == "'":
                if i + 1 < len(raw) and raw[i + 1] == "'":
                    buf.append("'"); i += 2
                else:
                    in_str = False; i += 1
            else:
                buf.append(ch); i += 1
        else:
            if ch == "'": in_str = True; i += 1
            elif ch == ",": out.append("".join(buf).strip()); buf = []; i += 1
            else: buf.append(ch); i += 1
    out.append("".join(buf).strip())
    def _norm(v):
        v = v.strip()
        if v.upper() == "NULL": return ""
        if v.startswith("'") and v.endswith("'"):
            return v[1:-1].replace("''","'")
        return v
    return [_norm(v) for v in out]

def _parse_sql_file(sql_path: Path) -> List[Dict[str, Any]]:
    """è¿”å›æ‰€æœ‰ INSERT è®°å½•ç»„æˆçš„ dict åˆ—è¡¨"""
    txt = _safe_read_sql(sql_path)
    entities = []
    for m in INSERT_RE.finditer(txt):
        cols = [c.strip().strip('"') for c in m.group("cols").split(",")]
        vals = _parse_values(m.group("vals"))
        if len(cols) != len(vals):
            continue
        entities.append(dict(zip(cols, vals)))
    return entities

def _ensure_entity_table(conn):
    with conn.cursor() as cur:
        if is_pg():
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS entity (
                    id BIGSERIAL PRIMARY KEY,
                    uuid VARCHAR(64) NOT NULL,
                    sid VARCHAR(64) NOT NULL,
                    name VARCHAR(255) NOT NULL,
                    type VARCHAR(128) NOT NULL,
                    data JSONB NOT NULL,
                    del SMALLINT DEFAULT 0,
                    input_date BIGINT DEFAULT 0,
                    update_date BIGINT DEFAULT 0
                );
                """
            )
            cur.execute("CREATE INDEX IF NOT EXISTS idx_entity_type ON entity(type);")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_entity_sid ON entity(sid);")
        else:
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS entity (
                    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
                    uuid VARCHAR(64) NOT NULL,
                    sid VARCHAR(64) NOT NULL,
                    name VARCHAR(255) NOT NULL,
                    type VARCHAR(128) NOT NULL,
                    data JSON NOT NULL,
                    del TINYINT DEFAULT 0,
                    input_date BIGINT DEFAULT 0,
                    update_date BIGINT DEFAULT 0,
                    KEY idx_type (type),
                    KEY idx_sid (sid)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
                """
            )

def _make_uuid10() -> str:
    n = int(time.time() * 1e6)
    base36 = "0123456789abcdefghijklmnopqrstuvwxyz"
    s = ""
    while n:
        n, r = divmod(n, 36)
        s = base36[r] + s
    return (s or "0")[-10:].rjust(10, "0")

def insert_entities(rows: List[Tuple[str,str,str,str,str,int,int,int]]):
    """rows: (uuid, sid, type, name, data_json, del, input_ts, update_ts)"""
    if not rows:
        return 0
    conn = pymysql.connect(**MYSQL_CFG)
    try:
        _ensure_entity_table(conn)
        sql = """
          INSERT INTO entity (`uuid`,`sid`,`type`,`name`,`data`,`del`,`input_date`,`update_date`)
          VALUES (%s,%s,%s,%s,%s,%s,%s,%s)
        """
        with conn.cursor() as cur:
            cur.executemany(sql, rows)
        conn.commit()
        return len(rows)
    except Exception as e:
        conn.rollback()
        print("[insert_entities error]", e)
        return 0
    finally:
        conn.close()
# ========== NEW: è§£æ type(key)[excludes] ==========
def _parse_type_and_key(type_spec: str) -> Tuple[str, str, List[str]]:
    """
    æ”¯æŒå¦‚ä¸‹å½¢å¼ï¼š
      - 'fund'                     -> (type='fund', key='id', excludes=[])
      - 'fund(usci)'               -> (type='fund', key='usci', excludes=[])
      - 'fund(usci)[data.id,name]' -> (type='fund', key='usci', excludes=['data.id','name'])
    [] ä¸­çš„æ’é™¤é¡¹ä»¥é€—å·åˆ†éš”ï¼›'name' è¡¨ç¤ºæ’é™¤é¡¶å±‚å®ä½“ nameï¼›'data.X' è¡¨ç¤ºæ’é™¤ data JSON ä¸­çš„å­—æ®µï¼ˆæ”¯æŒ a.b.c è·¯å¾„ï¼‰ã€‚
    """
    s = (type_spec or "").strip()
    m = re.match(r"^(?P<typ>\w+)(?:\((?P<key>[\w\.]+)\))?(?:\[(?P<excl>[^\]]+)\])?$", s)
    if not m:
        return s, "id", []
    t = (m.group("typ") or "").strip()
    k = (m.group("key") or "id").strip()
    excl_raw = (m.group("excl") or "").strip()
    excludes: List[str] = []
    if excl_raw:
        excludes = [x.strip() for x in excl_raw.split(",") if x.strip()]
    return t, k, excludes

def _del_by_path(obj: Dict[str, Any], path: str):
    """ä»å­—å…¸ obj ä¸­åˆ é™¤è·¯å¾„ pathï¼ˆä¾‹å¦‚ 'a.b.c'ï¼‰ï¼Œè‹¥ä¸å­˜åœ¨åˆ™å¿½ç•¥ã€‚"""
    if not path:
        return
    parts = path.split(".")
    cur = obj
    for p in parts[:-1]:
        if not isinstance(cur, dict):
            return
        if p not in cur:
            return
        cur = cur[p]
    if isinstance(cur, dict) and parts[-1] in cur:
        try:
            del cur[parts[-1]]
        except Exception:
            pass


# ========== NEW: æŠ½å–é¡¶å±‚ metaï¼ˆå¹¶ä» data JSON ä¸­å‰”é™¤è¿™äº› metaï¼‰==========
def _extract_entity_meta(mapped: Dict[str, Any], now_ts: int) -> Dict[str, int]:
    """
    å°† del / input_date / update_date ä½œä¸º entity é¡¶å±‚åˆ—ä½¿ç”¨ï¼›
    è‹¥ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼›ä» mapped ä¸­ç§»é™¤è¿™äº›é”®ï¼Œé¿å…è¿›å…¥ data JSONã€‚
    """
    meta = {
        "del": int(mapped.pop("del", 0) or 0),
        "input_date": int(mapped.pop("input_date", now_ts) or now_ts),
        "update_date": int(mapped.pop("update_date", now_ts) or now_ts),
    }
    return meta


# ========== NEW: å•æ¡ UPSERT åˆ° entity ==========
def _upsert_entity_row(type_name: str, key_field: str, key_value: Any,
                       sid: str, name_val: str, data_json: str,
                       meta: Dict[str, int], import_mode: str = "upsert") -> int:
    """
    ä½¿ç”¨ (type_name, JSON_EXTRACT(data, $.<key_field>)=key_value) å®ç° UPSERTã€‚
    å‘½ä¸­åˆ™ UPDATEï¼šnameã€dataã€update_dateï¼›æœªå‘½ä¸­åˆ™ INSERTã€‚
    è¿”å› 1 è¡¨ç¤ºæˆåŠŸå†™å…¥ï¼ˆæ— è®ºæ’å…¥è¿˜æ˜¯æ›´æ–°ï¼‰ã€‚
    """
    if key_value in (None, ""):
        # æ²¡æœ‰å”¯ä¸€é”®å€¼ï¼Œä¸å†™
        return 0

    conn = get_conn()
    try:
        with conn.cursor() as cur:
            # æŸ¥è¯¢æ˜¯å¦å·²å­˜åœ¨è¯¥å”¯ä¸€é”®
            sel_sql = f"""
                SELECT uuid, name, data FROM entity
                WHERE type=%s AND {json_equals_clause('data', key_field)}
                LIMIT 1
            """
            cur.execute(sel_sql, (type_name, str(key_value)))
            row = cur.fetchone()

            if row:  # å‘½ä¸­
                uuid, old_name, old_data_json = row[0], row[1], row[2]
                # åˆå¹¶ç­–ç•¥ï¼šå½“ import_mode ä¸º upsert æˆ– update_only æ—¶ä¹Ÿä½¿ç”¨åˆå¹¶ï¼Œé¿å…è¦†ç›–ä¸¢å­—æ®µ
                # æ—§æ•°æ®è§£æå¤±è´¥åˆ™å›é€€ä¸ºæ›¿æ¢
                merged_json = data_json
                try:
                    old_data = json.loads(old_data_json or "{}")
                    new_data = json.loads(data_json or "{}")
                    def deep_merge(a, b):
                        for k, v in b.items():
                            if isinstance(v, dict) and isinstance(a.get(k), dict):
                                deep_merge(a[k], v)
                            else:
                                a[k] = v
                        return a
                    merged = deep_merge(old_data, new_data)
                    merged_json = json.dumps(merged, ensure_ascii=False)
                except Exception:
                    merged_json = data_json

                if import_mode in ("upsert", "update_only", "upsert_merge", "update_merge"):
                    # è‹¥ name ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä¿ç•™æ—§ name
                    final_name = old_name if (name_val is None or str(name_val) == "") else name_val
                    upd_sql = """
                        UPDATE entity
                        SET name=%s,
                            data=%s,
                            del=%s,
                            update_date=%s
                        WHERE uuid=%s
                    """
                    cur.execute(
                        upd_sql,
                        (final_name, merged_json, int(meta["del"]), int(meta["update_date"]), uuid)
                    )
                else:
                    # create_only å‘½ä¸­åˆ™ä¸å†™
                    return 0
            else:    # æœªå‘½ä¸­
                if import_mode in ("upsert", "create_only"):
                    ins_sql = """
                        INSERT INTO entity
                            (uuid,sid,type,name,data,del,input_date,update_date)
                        VALUES
                            (%s,%s,%s,%s,%s,%s,%s,%s)
                    """
                    cur.execute(
                        ins_sql,
                        (_make_uuid10(), sid, type_name, name_val, data_json,
                         int(meta["del"]), int(meta["input_date"]), int(meta["update_date"]))
                    )
                else:
                    # update_only æœªå‘½ä¸­åˆ™ä¸å†™
                    return 0

        conn.commit()
        return 1
    except Exception as e:
        conn.rollback()
        print("[_upsert_entity_row error]", e)
        return 0
    finally:
        conn.close()

def upsert_entity(type_name: str, key_field: str, key_value: Any, name: str, data_json: str):
    conn = get_conn()
    try:
        with conn.cursor() as cur:
            # 1) æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒä¸€ fund
            sql_sel = f"""
                SELECT uuid FROM entity
                WHERE type=%s AND {json_equals_clause('data', key_field)} LIMIT 1
            """
            cur.execute(sql_sel, (type_name, str(key_value)))
            row = cur.fetchone()

            now_ts = int(time.time())
            if row:  # å­˜åœ¨ â†’ UPDATE
                uuid = row[0]
                sql_upd = """
                    UPDATE entity
                    SET name=%s, data=%s, update_date=%s
                    WHERE uuid=%s
                """
                cur.execute(sql_upd, (name, data_json, now_ts, uuid))
            else:   # ä¸å­˜åœ¨ â†’ INSERT
                uuid = _make_uuid10()
                sql_ins = """
                    INSERT INTO entity(uuid, sid, type, name, data, del, input_date, update_date)
                    VALUES (%s,%s,%s,%s,%s,0,%s,%s)
                """
                cur.execute(sql_ins, (uuid, SID, type_name, name, data_json, now_ts, now_ts))

        conn.commit()
        return 1
    except Exception as e:
        conn.rollback()
        print("[upsert_entity error]", e)
        return 0
    finally:
        conn.close()

# =============== å¯¹å¤–ï¼šçŠ¶æ€ / å…¥åº“ / åˆ é™¤ ===============
def check_entity_status(type_name: str, sid: Optional[str] = None) -> int:
    """è¿”å›è¯¥ type åœ¨ entity çš„è®°å½•æ•°ï¼›è‹¥ä¼ å…¥ sid åˆ™æŒ‰ sid è¿‡æ»¤"""
    conn = get_conn()
    try:
        with conn.cursor() as cur:
            if sid:
                cur.execute("SELECT COUNT(*) FROM entity WHERE type=%s AND sid=%s", (type_name, sid))
            else:
                cur.execute("SELECT COUNT(*) FROM entity WHERE type=%s", (type_name,))
            n = cur.fetchone()[0]
            return int(n or 0)
    except Exception as e:
        print("[check_entity_status error]", e)
        return 0
    finally:
        conn.close()

def import_table_data(source_table: str, sid: str = None, target_entity_spec: Optional[str] = None, import_mode: str = "upsert", progress_cb: Optional[Callable[[int, int], None]] = None) -> int:
    """
    è¯»å– source/sql/<table>.sql çš„æ‰€æœ‰ INSERTï¼Œæ˜ å°„åæŒ‰ (type(key)) è§„åˆ™ UPSERT å…¥åº“ã€‚
    - target_entity æ”¯æŒ 'fund' æˆ– 'fund(id)'ï¼›åè€…è¡¨ç¤ºç»Ÿä¸€ä¸»é”®æ˜¯ data.idã€‚
    - è‹¥æœªé…ç½® ()ï¼Œé»˜è®¤ key_field='id'ã€‚
    - ç¡®ä¿è¯¥ key_field å†™å…¥åˆ° data JSONï¼ˆå³ mapped_data[key_field] å­˜åœ¨ï¼‰ã€‚
    - del/input_date/update_date æŠ½åˆ° entity é¡¶å±‚ï¼ˆä¸è¿› data JSONï¼‰ã€‚
    """
    sid = sid or SID
    sql_path = detect_sql_path(source_table)
    if not sql_path.exists():
        print(f"[import_table_data] SQL not found: {sql_path}")
        return 0

    records = _parse_sql_file(sql_path)
    if not records:
        print(f"[import_table_data] No INSERT values in {sql_path.name}")
        return 0

    # â­ æ”¯æŒå¤–éƒ¨æŒ‡å®šç›®æ ‡ç±»å‹ä¸æ’é™¤ï¼ˆå¦‚ 'fund'ã€'fund(id)'ã€'fund(usci)[data.id,name]'ï¼‰
    target_type_spec = (target_entity_spec or get_target_entity(source_table) or source_table)
    final_type, key_field, excludes = _parse_type_and_key(target_type_spec)

    now_ts = int(time.time())
    wrote = 0
    total = len(records)
    # è¿›åº¦å›è°ƒèŠ‚æµï¼šæœ€å¤š ~100 æ¬¡æ›´æ–°ï¼Œä¸”ä¿è¯æœ€åä¸€æ¬¡æ›´æ–°
    stride = 1 if total <= 100 else max(1, total // 100)
    last_cb_ts = 0.0

    for idx, rec in enumerate(records, start=1):
        # 1) æ˜ å°„ï¼šæŒ‰å½“å‰ final_type è¿‡æ»¤å­—æ®µæ˜ å°„ & è„šæœ¬ä¸Šä¸‹æ–‡
        mapped_data, out_name, type_override = apply_record_mapping(
            source_table, rec, py_script="", target_entity=final_type
        )
        type_here = (type_override or final_type).strip() or source_table

        # 2) ç»Ÿä¸€é”®å€¼ï¼šä¼˜å…ˆ mapped_dataï¼Œå…¶æ¬¡åŸå§‹ rec
        key_val = mapped_data.get(key_field, None)
        if key_val in (None, ""):
            key_val = rec.get(key_field, "")

        # 3) åº”ç”¨æ’é™¤ï¼šåˆ é™¤æŒ‡å®šçš„ data.* å­—æ®µï¼›'name' æ’é™¤åˆ™ä¸å†™é¡¶å±‚ name
        name_excluded = False
        for ex in excludes:
            if ex == "name":
                name_excluded = True
            elif ex.startswith("data."):
                p = ex[5:].strip()
                if p and p != key_field:
                    _del_by_path(mapped_data, p)

        # 4) ç¡®ä¿è¯¥ç»Ÿä¸€é”®è¢«å†™å…¥ data JSON
        if key_field not in mapped_data:
            mapped_data[key_field] = key_val

        # 5) å…ƒå­—æ®µæŠ½å–ï¼ˆå¹¶ä» data JSON å‰”é™¤ï¼‰
        meta = _extract_entity_meta(mapped_data, now_ts=now_ts)

        # 6) name å– mapped_data['__name__'] å›è½ out_nameï¼›è‹¥æ’é™¤ name åˆ™ç½®ç©º
        name_val = (mapped_data.get("__name__") or out_name or "").strip()
        if name_excluded:
            name_val = ""
            if "__name__" in mapped_data:
                try:
                    del mapped_data["__name__"]
                except Exception:
                    pass

        # 7) åºåˆ—åŒ– dataï¼ˆæ­¤æ—¶å·²ä¸åŒ…å« del/input_date/update_dateï¼‰
        data_json = json.dumps(mapped_data, ensure_ascii=False)

        # 8) UPSERT
        wrote += _upsert_entity_row(
            type_name=type_here,
            key_field=key_field,
            key_value=key_val,
            sid=sid,
            name_val=name_val,
            data_json=data_json,
            meta=meta,
            import_mode=import_mode
        )
        # 9) è¿›åº¦å›è°ƒï¼ˆæ¯å¤„ç†ä¸€æ¡è®°å½•ï¼‰
        try:
            if progress_cb:
                now = time.time()
                if (idx == total) or (idx % stride == 0) or (now - last_cb_ts >= 0.05):
                    progress_cb(idx, total)
                    last_cb_ts = now
        except Exception:
            # å›è°ƒå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass

    return wrote

def delete_table_data(type_name: str, sid: Optional[str] = None) -> int:
    """ä» entity ç‰©ç†åˆ é™¤è¯¥ typeï¼ˆæŒ‰å½“å‰ sid é™åˆ¶ï¼‰"""
    conn = get_conn()
    try:
        with conn.cursor() as cur:
            # è‹¥æœªæ˜¾å¼ä¼ å…¥ï¼Œå›é€€åˆ°å…¨å±€ SID
            cur_sid = sid or SID
            cur.execute("SELECT COUNT(*) FROM entity WHERE type=%s AND sid=%s", (type_name, cur_sid))
            n = int(cur.fetchone()[0] or 0)
            cur.execute("DELETE FROM entity WHERE type=%s AND sid=%s", (type_name, cur_sid))
        conn.commit()
        return n
    except Exception as e:
        conn.rollback()
        print("[delete_table_data error]", e)
        return 0
    finally:
        conn.close()




# ==================== å…¶å®ƒå·¥å…· ====================
def get_all_prioritized_tables() -> List[str]:
    rows = list_tables(include_disabled=False)
    return [r[0] for r in rows]

def get_table_priority(source_table: str) -> int:
    return get_priority(source_table)
def _extract_entity_meta(mapped: Dict[str, Any], now_ts: Optional[int] = None) -> Dict[str, Any]:
    """
    ä»æ˜ å°„å®Œçš„ new_rec ä¸­æŠ½å‡ºéœ€è¦æ”¾åˆ° entity é¡¶å±‚çš„å…ƒå­—æ®µï¼š
      - del               -> é¡¶å±‚ del (int)
      - input_date        -> é¡¶å±‚ input_date (bigint ç§’/æ¯«ç§’éƒ½å¯ï¼Œä½ è¿™é‡Œç”¨ç§’)
      - update_date       -> é¡¶å±‚ update_date (bigint)
    æŠ½å‡ºåä¼šä» mapped ä¸­ç§»é™¤è¿™äº›é”®ï¼Œä»¥ä¿è¯ data é‡Œä¸å†åŒ…å«å®ƒä»¬ã€‚
    """
    meta = {}
    if "del" in mapped:
        try:
            meta["del"] = int(mapped.pop("del"))
        except Exception:
            meta["del"] = 0
    if "input_date" in mapped:
        try:
            meta["input_date"] = int(mapped.pop("input_date"))
        except Exception:
            meta["input_date"] = int(now_ts or time.time())
    if "update_date" in mapped:
        try:
            meta["update_date"] = int(mapped.pop("update_date"))
        except Exception:
            meta["update_date"] = int(now_ts or time.time())

    # é»˜è®¤å…œåº•
    if "del" not in meta:
        meta["del"] = 0
    if "input_date" not in meta:
        meta["input_date"] = int(now_ts or time.time())
    if "update_date" not in meta:
        meta["update_date"] = int(now_ts or time.time())
    return meta